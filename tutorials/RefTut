###dDocent Reference Assembly Tutorial###
###Designed by Jon Puritz###

###GOALS###
##	1.  To set up test data for our exercise
##	2.	To demultiplex samples with process_radtags and rename samples 
##	3.	To use the methods of dDocent (via rainbow) to assemble reference contigs
##	4.	To learn how optimize a de novo reference assembly
##	5.	To learn how to utilize pyRAD to .
###########

## This tutorial is executable.  Simply type RefTut [number] to skip directly to the numbered
## step of the tutorial.
## Command lines start with a "$"


#1 Let's get started.  First let's create a working directory.

#1		$mkdir D1W

#2 Let's change into that directory

#c#2	$cd D1W

#3 Now let's get the test data I created for the course.

#c#3    $curl -L -o data.zip https://www.dropbox.com/s/t09xjuudev4de72/data.zip?dl=0

#4 Let's check that everything went well.

#c#4	$unzip data.zip && ll 

#4 You should see something like this:
#4 total 7664
#4 -rw-rw-r--. 1 j.puritz j.puritz 3127907 Mar  1 10:26 data.zip
#4 -rwxr--r--. 1 j.puritz j.puritz     600 Mar  6  2015 SimRAD.barcodes
#4 -rwxr--r--. 1 j.puritz j.puritz 2574784 Mar  6  2015 SimRAD_R1.fastq.gz
#4 -rwxr--r--. 1 j.puritz j.puritz 2124644 Mar  6  2015 SimRAD_R2.fastq.gz
#4 -rwxr--r--. 1 j.puritz j.puritz   12272 Mar  6  2015 simRRLs2.py
#5
#5  The data that we are going to use was simulated using the simRRLs2.py script that I modified from the one published by Deren Eaton.  
#5 You can find the original version here (http://dereneaton.com/software/simrrls/).  Basically, the script simulated ddRAD 1000 loci 
#5 shared across an ancestral population and two extant populations.  Each population had 180,000 individuals, and the two extant 
#5 population split from the ancestral population 576,000 generations ago and split from each other 288,000 generation ago.  The two 
#5 populations exchanged 4N*0.001 migrants per generation until about 2,000 generations ago.  4Nu equaled 0.00504 and mutations had a 10% 
#5 chance of being an INDEL polymorphism.  Finally, reads for each locus were simulated on a per individual basis at a mean of 20X 
#5 coverage (coming from a normaldistribution with a SD 8) and had an inherent sequencing error rate of 0.001. 
#5
#5 In short, we have two highly polymorphic populations with only slight levels of divergence from each other.  GST should be approximately
#5 0.005. The reads are contained in the two fastq.gz files.
#5
#5 Let's go ahead and demultiplex the data.  This means we are going to separate individuals by barcode.
#5 My favorite software for this task is process_radtags from the Stacks package (http://creskolab.uoregon.edu/stacks/)
#5
#5 process_radtags takes fastq or fastq.gz files as input along with a file that lists barcodes.  Data can be separated according to inline
#5 barcodes (barcodes in the actual sequence), Illumina Index, or any combination of the two.  Check out the manual at this website
#5 (http://creskolab.uoregon.edu/stacks/comp/process_radtags.php)
#5
#5 Let's start by making a list of barcodes.  The SimRAD.barcodes file actually has the sample name and barcode listed.  See for yourself.
#c#5	$head SimRAD.barcodes
#5
#5 You should see:
#5 PopA_01 ATGGGG
#5 PopA_02 GGGTAA
#5 PopA_03 AGGAAA
#5 PopA_04 TTTAAG
#5 PopA_05 GGTGTG
#5 PopA_06 TGATGT
#5 PopA_07 GGTTGT
#5 PopA_08 ATAAGT
#5 PopA_09 AAGATA
#5 PopA_10 TGTGAG
#5 
#5 We need to turn this into a list of barcodes.  We can do this easily with the cut command.
#c#5	$cut -f2 SimRAD.barcodes > barcodes
#5
#5 Now we have a list of just barcodes.  The cut command let's you select a column of text with the -f (field command).
#5 We used -f2 to get the second column.  
#c#5	$head barcodes
#6
#6 Now we can run process_radtags
#c#6	$process_radtags -1 SimRAD_R1.fastq.gz -2 SimRAD_R2.fastq.gz -b barcodes -e ecoRI --renz_2 mspI -r -i gzfastq
#6 The option -e specifies the 5' restriction site and --renze_2 species the 3' restriction site.  -i states the format of the input 
#6 sequences.The -r option tells the program to fix cut sites and barcodes that have up to 1-2 mutations in them.  This can be changed 
#6 with the --barcode_dist flag.  
#6 
#6 Once the program is completed.  Your output directory should have several files that look like: sample_AAGAGG.1.fq.gz
#6 sample_AAGAGG.2.fq.gz, sample_AAGAGG.rem.1.fq.gz, and sample_AAGAGG.rem.2.fq.gz
#7
#7 The *.rem.*.fq.gz files would normally have files that fail process_radtags (bad barcode, ambitious cut sites), but we have 
#7 simulated data and none of those bad reads.  We can delete.
#7
#c#7	$rm *rem*
#8
#8 The individual files are currently only names by barcode sequence.  We can rename them in an easier convention using a simple bash script.
#8 Download the "Rename_for_dDocent.sh" script from my github repository
#8
#c#8	$curl -L -O https://github.com/jpuritz/dDocent/raw/master/Rename_for_dDocent.sh
#9
#9 Take a look at this simple script
#c#9	$cat Rename_for_dDocent.sh
#9
#9 Bash scripts are a wonderful tool to automate simple tasks.  This script begins with an If statement to see if a file was provided as 
#9 input.  If the file is not it exits and says why.  The file it requires is a two column list with the sample name in the first column 
#9 and sample barcode in the second column.  The script reads all the names into an array and all the barcodes into a second array, and 
#9 then gets the length of both arrays.  It then iterates with a for loop the task of renaming the samples.  
#10
#10 Now run the script to rename your samples and take a look at the output
#c#10	$bash Rename_for_dDocent.sh SimRAD.barcodes
#c#10	$ls *.fq.gz
#10 There should now be 40 individually labeled .F.fq.gz and 40 .R.fq.gz.  Twenty from PopA and Twenty from PopB.
#10 Now we are ready to rock!
#11
#11 Let's start by examining how the dDocent pipeline assembles RAD data.
#11 First, we are going to create a set of uniq reads with counts for each individuals
#c#11	$ls *.F.fq.gz > namelist
#c#11	$sed -i'' -e 's/.F.fq.gz//g' namelist
#c#11	$AWK1='BEGIN{P=1}{if(P==1||P==2){gsub(/^[@]/,">");print}; if(P==4)P=0; P++}'
#c#11	$AWK2='!/>/'
#c#11	$AWK3='!/NNN/'
#c#11	$PERLT='while (<>) {chomp; $z{$_}++;} while(($k,$v) = each(%z)) {print "$v\t$k\n";}'
#11
#c#11	$cat namelist | parallel --no-notice -j 8 "zcat {}.F.fq.gz | mawk '$AWK1' | mawk '$AWK2' > {}.forward"
#c#11	$cat namelist | parallel --no-notice -j 8 "zcat {}.R.fq.gz | mawk '$AWK1' | mawk '$AWK2' > {}.reverse"
#c#11	$cat namelist | parallel --no-notice -j 8 "paste -d '-' {}.forward {}.reverse | mawk '$AWK3' | sed 's/-/NNNNNNNNNN/' | perl -e '$PERLT' > {}.uniq.seqs"
#12
#12 The first four lines simply set shell variables for various bits of AWK and perl code, 
#12 to make parallelization with GNU-parallel easier. The first line after the variables, 
#12 creates a set of forward reads for each individual by using mawk (a faster, c++ version of awk) 
#12 to sort through the fastq file and strip away the quality scores.  The second line does the same for the PE reads.  
#12 Lastly, the final line concatentates the forward and PE reads together (with 10 Ns between them) and then find the 
#12 unique reads within that individual and counts the occurences (coverage).
#12
#12 Now we can take advantage of some of the properties for RAD sequencing.  
#12 Sequences with very small levels of coverage within an individual are likely to be sequencing errors.  
#12 So, for assembly, we can eliminate reads with low copy numbers to remove non-informative data!
#12
#12 Let's sum up the number the within individual coverage level of unique reads in our data set
#c#12	$cat *.uniq.seqs > uniq.seqs
#c#12	$for i in {2..20};
#c#12	$do 
#c#12	$echo $i >> pfile
#c#12	$done
#c#12	$cat pfile | parallel --no-notice "echo -n {}xxx && mawk -v x={} '\$1 >= x' uniq.seqs | wc -l" | mawk  '{gsub("xxx","\t",$0); print;}'| sort -g > uniqseq.data
#c#12	$rm pfile
#12
#12 This is another example of a BASH for loop.  It uses mawk to query the first column and
#12 select data above a certain copy number (from 2-20) and prints that to a file.
#13
#13 Take a look at the contents of uniqseq.data
#c#13	$more uniqseq.data
#14 We can even plot this to the terminal using gnuplot
#c#14	$gnuplot << \EOF 
#c#14	$set terminal dumb size 120, 30
#c#14	$set autoscale
#c#14	$set xrange [2:20] 
#c#14	$unset label
#c#14	$set title "Number of Unique Sequences with More than X Coverage (Counted within individuals)"
#c#14	$set xlabel "Coverage"
#c#14	$set ylabel "Number of Unique Sequences"
#c#14	$plot 'uniqseq.data' with lines notitle
#c#14	$pause -1
#c#14	$EOF
#14
#14 The graph should look like:
#14                    Number of Unique Sequences with More than X Coverage (Counted within individuals)
#14 Number of Unique Sequences
#14  70000 ++----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+----------++
#14        +           +           +           +           +          +           +           +           +           +
#14        |                                                                                                          |
#14  60000 ******                                                                                                    ++
#14        |     ******                                                                                               |
#14        |           ******                                                                                         |
#14        |                 ******                                                                                   |
#14  50000 ++                      *****                                                                             ++
#14        |                            *                                                                             |
#14        |                             *****                                                                        |
#14  40000 ++                                 *                                                                      ++
#14        |                                   ******                                                                 |
#14        |                                         *****                                                            |
#14        |                                              *                                                           |
#14  30000 ++                                              *****                                                     ++
#14        |                                                    *                                                     |
#14        |                                                     *****                                                |
#14  20000 ++                                                         ******                                         ++
#14        |                                                                ******                                    |
#14        |                                                                      ******                              |
#14        |                                                                            ******                        |
#14  10000 ++                                                                                 ************           ++
#14        |                                                                                              *************
#14        +           +           +           +           +          +           +           +           +           +
#14      0 ++----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+----------++
#14        2           4           6           8           10         12          14          16          18          20
#14                                                         Coverage
#14
#15 Now we need to choose a cutoff value.
#15 We want to choose a value that captures as much of the diversity of the data as possible 
#15 while simultaneously eliminating sequences that are likely errors.
#15 Let's try 4
#c#15	$parallel --no-notice -j 8 mawk -v x=4 \''$1 >= x'\' ::: *.uniq.seqs | cut -f2 | perl -e 'while (<>) {chomp; $z{$_}++;} while(($k,$v) = each(%z)) {print "$v\t$k\n";}' > uniqCperindv
#c#15	$wc -l uniqCperindv
#15 We've now reduced the data to assemble down to 7598 sequences!
#16 But, we can go even further.
#16 Let's now restrict data by the number of different individuals a sequence appears within.
#c#16	$for ((i = 2; i <= 10; i++));
#c#16	$do
#c#16	$echo $i >> ufile
#c#16	$done
#16
#c#16	$cat ufile | parallel --no-notice "echo -n {}xxx && mawk -v x={} '\$1 >= x' uniqCperindv | wc -l" | mawk  '{gsub("xxx","\t",$0); print;}'| sort -g > uniqseq.peri.data
#c#16	$rm ufile
#17 Again, we can plot the data:
#c#17	$gnuplot << \EOF 
#c#17	$set terminal dumb size 120, 30
#c#17	$set autoscale 
#c#17	$unset label
#c#17	$set title "Number of Unique Sequences present in more than X Individuals"
#c#17	$set xlabel "Number of Individuals"
#c#17	$set ylabel "Number of Unique Sequences"
#c#17	$plot 'uniqseq.peri.data' with lines notitle
#c#17	$pause -1
#c#17	$EOF
#17
#17 The graph should look like:
#17                                 Number of Unique Sequences present in more than X Individuals
#17  Number of Unique Sequences
#17    6000 ++------------+------------+-------------+------------+-------------+------------+-------------+-----------++
#17         +             +            +             +            +             +            +             +            +
#17         |                                                                                                           |
#17    5500 *****                                                                                                      ++
#17         |    **                                                                                                     |
#17    5000 ++     ***                                                                                                 ++
#17         |         ***                                                                                               |
#17         |            *                                                                                              |
#17    4500 ++            *****                                                                                        ++
#17         |                  ****                                                                                     |
#17         |                      ***                                                                                  |
#17    4000 ++                        *                                                                                ++
#17         |                          ***********                                                                      |
#17    3500 ++                                    ***                                                                  ++
#17         |                                        **********                                                         |
#17         |                                                  ***                                                      |
#17    3000 ++                                                    ***********                                          ++
#17         |                                                                ***                                        |
#17         |                                                                   *************                           |
#17    2500 ++                                                                               **************            ++
#17         |                                                                                              *************|
#17    2000 ++                                                                                                         +*
#17         |                                                                                                           |
#17         +             +            +             +            +             +            +             +            +
#17    1500 ++------------+------------+-------------+------------+-------------+------------+-------------+-----------++
#17         2             3            4             5            6             7            8             9            10
#17                                                     Number of Individuals
#17
#18 Again, we need to choose a cutoff value.
#18 We want to choose a value that captures as much of the diversity of the data as possible 
#18 while simultaneously eliminating sequences that have little value on the population scale.
#18 Let's try 4.
#c#18	$mawk -v x=4 '$1 >= x' uniqCperindv > uniq.k.4.c.4.seqs
#c#18	$wc -l uniq.k.4.c.4.seqs
#18
#18 Now we have reduced the data down to only 3840 sequences!
#19
#19 Let's quickly convert these sequences back into fasta format
#19 We can do this with two quick lines of code:
#c#19	$cut -f2 uniq.k.4.c.4.seqs > totaluniqseq
#c#19	$mawk '{c= c + 1; print ">Contig_" c "\n" $1}' totaluniqseq > uniq.fasta
#19 This simple script reads the totaluniqseq file line by line and add a sequence header of >Contig X
#19
#19 At this point, dDocent also checks for reads that have a substantial amount of Illumina adapter in them.  
#19 Our data is simulated and does not contain adapter, so we'll skip that step for the time being.
#20
#20 With this, we have created our reduced data set and are ready to start assembling reference contigs.
#20 First, let's extract the forward reads.
#c#20	$sed -e 's/NNNNNNNNNN/\t/g' uniq.fasta | cut -f1 > uniq.F.fasta
#20 
#20 This uses the sed command to replace the 10N separator into a tab character and then uses the cut
#20 function to split the files into forward reads.
#21
#21 Previous versions of dDocent utilized the program rainbow to do full RAD assembly; 
#21 however, as of dDocent 2.0, parts of rainbow have been replaced for better functionality.  
#21 For example, first step of rainbow clusters reads together using a spaced hash to estimate 
#21 similarity in the forward reads only.  
#21 dDocent now improves this by using clustering by alignment via the program CD-hit to achieve more accurate clustering.  
#21 Custom AWK code then converts the output of CD-hit to match the input of the 2nd phase of rainbow.
#21
#c#21	$cd-hit-est -i uniq.F.fasta -o xxx -c 0.8 -T 0 -M 0 -g 1
#21
#21 This code clusters all off the forward reads by 80% similarity.  
#21 This might seem low, but other functions of rainbow will break up clusters given the number and frequency of variants, 
#21 so it's best to use a low value at this step.
#22
#c#22	$mawk '{if ($1 ~ /Cl/) clus = clus + 1; else  print $3 "\t" clus}' xxx.clstr | sed 's/[>Contig_,...]//g' | sort -g -k1 > sort.contig.cluster.ids
#c#22	$paste sort.contig.cluster.ids totaluniqseq > contig.cluster.totaluniqseq
#c#22	$sort -k2,2 -g contig.cluster.totaluniqseq | sed -e 's/NNNNNNNNNN/\t/g' > rcluster
#22 This code then converts the output of CD-hit to match the output of the first phase of rainbow.
#22 The output follows a simple text format of:
#22 Read_ID	Cluster_ID	Forward_Read	Reverse_Read
#23
#23 Use the more, head, and/or tail function to examine the output file (rcluster)
#23 You should see approximately 1000 as the last cluster.  
#23 It's important to note that the numbers are not totally sequential and that there may not be 1000 clusters.  
#23 Try the command below to get the exact number.
#23
#c#23	$cut -f2 rcluster | uniq | wc -l 
#23
#23 The actual number of clusters is 1000 in this case because this is simulated data.  
#24
#24 The next step of rainbow is to split clusters formed in the first step into smaller clusters 
#24 representing significant variants.
#24 Think of it in this way.  The first clustering steps found RAD loci, and this step is splitting the loci into alleles. 
#24 This *also* helps to break up over clustered sequences.
#c#24	$rainbow div -i rcluster -o rbdiv.out 
#25
#25 The output of the div process is similar to the previous output with the exception that the second column is now the new divided cluster_ID
#25 (this value is numbered sequentially) and there was a column added to the end of the file that holds the original first cluster ID
#25 The parameter -f can be set to control what is the minimum frequency of an allele necessary to divide it into its own cluster
#25 The -K parameter controls the minimum number of alleles to split regardless of frequency.
#25
#c#25	$rainbow div -i rcluster -o rbdiv.out -f 0.5 -K 10
#25
#25 Though changing the parameter for this data set has no effect, it can make a big difference when using real data.
#26
#26 The third part of the rainbow process is to used the paired end reads to merge divided clusters.  
#26 This helps to double check the clustering and dividing of the previous steps
#26 all of which were based on the forward read.  The logic is that if divided clusters represent alleles from the same homolgous locus, 
#26 they should have fairly similar paired end reads as well as forward.  Divided clusters that do not share similarity in the paired-end 
#26 read represent cluster paralogs or repetitive regions.  After the divided clusters are merged,
#26 all the forward and reverse reads are pooled and assembled for that cluster.
#26
#c#26	$rainbow merge -o rbasm.out -a -i rbdiv.out
#27
#27 A parameter of interest to add here is the -r parameter, which is the minimum number of reads to assemble.  
#27 The default is 5 which works well if assembling reads from a single individual.
#27 However, we are assembling a reduced data set, so there may only be one copy of a locus.  
#27 Therefore, it's more appropriate to use a cutoff of 2.
#c#27	$rainbow merge -o rbasm.out -a -i rbdiv.out -r 2
#28
#28 The rbasm output lists optimal and suboptimal contigs.  Previous versions of dDocent used rainbow's included perl scripts to retrieve 
#28 optimal contigs.  However, as of version 2.0, dDocent uses customized AWK code to extract optimal contigs for RAD sequencing.  
#28 
#c#28	$cat rbasm.out <(echo "E") |sed 's/[0-9]*:[0-9]*://g' | mawk ' {
#c#28	$if (NR == 1) e=$2;
#c#28	$else if ($1 ~/E/ && lenp > len1) {c=c+1; print ">dDocent_Contig_" e "\n" seq2 "NNNNNNNNNN" seq1; seq1=0; seq2=0;lenp=0;e=$2;fclus=0;len1=0;freqp=0;lenf=0}
#c#28	$else if ($1 ~/E/ && lenp <= len1) {c=c+1; print ">dDocent_Contig_" e "\n" seq1; seq1=0; seq2=0;lenp=0;e=$2;fclus=0;len1=0;freqp=0;lenf=0}
#c#28	$else if ($1 ~/C/) clus=$2;
#c#28	$else if ($1 ~/L/) len=$2;
#c#28	$else if ($1 ~/S/) seq=$2;
#c#28	$else if ($1 ~/N/) freq=$2;
#c#28	$else if ($1 ~/R/ && $0 ~/0/ && $0 !~/1/ && len > lenf) {seq1 = seq; fclus=clus;lenf=len}
#c#28	$else if ($1 ~/R/ && $0 ~/0/ && $0 ~/1/) {seq1 = seq; fclus=clus; len1=len}
#c#28	$else if ($1 ~/R/ && $0 ~!/0/ && freq > freqp && len >= lenp || $1 ~/R/ && $0 ~!/0/ && freq == freqp && len > lenp) {seq2 = seq; lenp = len; freqp=freq}
#c#28	$}' > rainbow.fasta
#28
#28 Now, this looks a bit complicated, but it's performing a fairly simple algorithm.  
#28 First, the script looks at all the contigs assembled for a cluster.  If any of the contigs contain forward and PE reads, then that contig is output as optimal.  
#28 If no overlap contigs exists (the usual for most RAD data sets), then the contig with the most assembled reads PE (most common) is output with the forward read contig with a 10 N spacer.  
#28 If two contigs have equal number of reads, the longer contig is output. 
#28
#28 At this point, dDocent (version 2.0 and higher) will check for substantial overlap between F and PE reads in the contigs.  
#28 Basically double checking rainbow's assembly.  We will skip this for our simulated data though.
#29
#29 Though rainbow is fairly accurate with assembly of RAD data, even with high levels of INDEL polymorphism.  
#29 It's not perfect and the resulting contigs need to be aligned and clustered by sequence similarity.  
#29 We can use the program cd-hit to do this.
#29
#c#29	$cd-hit-est -i rainbow.fasta -o referenceRC.fasta -M 0 -T 0 -c 0.9
#29
#29 The `-M` and `-T` flags instruct the program on memory usage (-M) and number of threads (-T).  
#29 Setting the value to 0 uses all available.  The real parameter of significan is the -c parameter 
#29 which sets the percentage of sequence similarity to group contigs by.  The above code uses 90%.  
#29 Try using 95%, 85%, 80%, and 99%. 
#29 Since this is simulated data, we know the real number of contigs, 1000.  
#29 By choosing an cutoffs of 4 and 4, we are able to get the real number of contigs, no matter what the similarty cutoff.  
#30
#30 In this example, it's easy to know the correct number of reference contigs, but with real data this is less obvious.  
#30 As you just demonstrated, varying the uniq sequence copy cutoff and the final clustering similarity have the
#30 the largest effect on the number of final contigs.  
#30 You could go back and retype all the steps from above to explore the data, but scripting makes this easier.
#30 I've made a simple bash script called remake_reference.sh that will automate the process.  
#c#30	$curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/remake_reference.sh
#31
#31 You can remake a reference by calling the script along with a new cutoff value and similarity.
#31
#c#31	$bash remake_reference.sh 4 4 0.90 PE 2
#31
#31 This command will remake the reference with a cutoff of 20 copies of a unique sequence to use for assembly and a final clustering value of 90%.
#31 It will output the number of reference sequences and create a new, indexed reference with the given parameters.
#31 The output from the code above should be "1000"
#31 Experiment with some different values on your own.   
#32
#32 What you choose for a final number of contigs will be something of a judgement call.  However, we could try to heuristically search the parameter space to find an optimal value.
#32 Download the script to automate this process.
#c#32	$curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/ReferenceOpt.sh
#33
#33 Take a look at the script ReferenceOpt.sh.  
#33 This script uses different loops to assemble references from an interval of cutoff values and c values from 0.8-0.98.  
#33 It take as a while to run, so I have pasted the output below for you.
#c#33	$#bash ReferenceOpt.sh 4 8 4 8 PE 16 #We aren't actually going to run this at the moment
#33
#33                                           Histogram of number of reference contigs
#33   Number of Occurrences
#33     200 ++--------------+--------------+---------------+--------------+---------------+--------------+--------------++
#33         +               +              +               +      'plot.kopt.data' using (bin($1,binwidth)):(1.0)*********
#33     180 ++                                                                                           *              +*
#33         |                                                                                            *               *
#33         |                                                                                            *               *
#33     160 ++                                                                                           *              +*
#33         |                                                                                            *               *
#33     140 ++                                                                                           *              +*
#33         |                                                                                            *               *
#33         |                                                                                            *               *
#33     120 ++                                                                                           *              +*
#33         |                                                                                            *               *
#33     100 ++                                                                                           *              +*
#33         |                                                                                            *               *
#33      80 ++                                                                                           *              +*
#33         |                                                                                            *               *
#33         |                                                                                            *               *
#33      60 ++                                                                                           *              +*
#33         |                                             ************************************************               *
#33      40 ++                                            *                                              *              +*
#33         |                                             *                                              *               *
#33         |                                             *                                              *               *
#33      20 ++                                            *                                              *              +*
#33         ***********************************************+              +               +              *               *
#33       0 **************************************************************************************************************
#33        988             990            992             994            996             998            1000            1002
#33                                                  Number of reference contigs
#33 
#33 Average contig number = 999.452
#33 The top three most common number of contigs
#33 X	Contig number
#33 164	1000
#33 19	998
#33 18	999
#33 The top three most common number of contigs (with values rounded)
#33 X	Contig number
#33 250	1000.0
#33
#33 You can see that the most common number of contigs across all iteration is 1000, 
#33 but also that the top three occuring and the average are all within 1% of the true value.
#33 Again, this is simulated data and with real data, the number of exact reference contigs is unknown and 
#33 you will ultimately have to make a judgement call.
#34
#34 Let's examine the reference a bit.
#c#34	$bash remake_reference.sh 4 4 0.90 PE 2
#c#34	$head reference.fasta
#34 >dDocent_Contig_1
#34 NAATTCCTCCGACATTGTCGGCTTTAAATAGCTCATAACTTGAGCCCAGGTAAAGACTTTAGTATACTCGCACCTTCCGCTTATCCCCCGGCCGCNNNNNNNNNNATTCAACCGCGGGACCTGAACTAACATAGCGTTGTGTATACCATCCGAGGTAACCTTATAACTCTCTGCCATTCGGACAGGTAACACGGCATATCGTCCGN
#34 >dDocent_Contig_2
#34 NAATTCAGAATGGTCATACAGGGCGGTAGAATGGAATCCTGAATCGAATGGCGGTTGCATTGAGAACCTGGTACCAGATAGGATCTGGATTAAATNNNNNNNNNNGTCGGGTACTAATTATCTATTGGGTCCAAACCCTCCGCCCCGTTTACTGCCCACCCGGCATGCAGTCATGAGAATTCCAAGGAACTAAGATAAGAGACCGN
#34 >dDocent_Contig_3
#34 NAATTCGGGCTCCTTGGAGAGATTCTTTCAATTATGCCCCCTACGTGGGAAACAGGGTCGGAAGTGGTCGGCTGAGAATTACTCGAAAGCCGCTCNNNNNNNNNNCCACCAGCATGATAGGACTTCAAGCTTGCCGTTTGTTGGGAGGACCGGTCGCTACGGAGCTGACGCTATCTCCCGCATCGGACCTCGTGGACAAAAACCGN
#34 >dDocent_Contig_4
#34 NAATTCAAAAGTCGCCCATAGGTACGTGATGAATTAGGTCAAGCGGGGACGTCGCATAGATGCGTGACGTCTGGAGCATGATGTTGTTTCTAACCNNNNNNNNNNAATCACTCGGTCAACGTGGTCCGTGCTCTGCAACGAAAAAAACTTCGCATGTGAACGATGATGCCTATAGGTGCGACCGCCGTCAGAGGCCCGTTGACCGN
#34 >dDocent_Contig_5
#34 NAATTCATACGGATATGATACTTCGTCTGGCAGGGTGGCTAGCGAGTTTAAGGATTCTTGGATAAAGGTAGGTAAAATTCTCGAGATTCTGATCTNNNNNNNNNNTAGAGGTGCTGGCGGGGCCTAGACGTGTTTCTACGCTTACTGATCAAATTAGCTAGCTTAGGTTCCTATAGTCTACGCTGGATTGTCCTTAGATGCACCGN
#34
#34 You can now see that we have complete RAD fragments starting with our EcoRI restriction site (AATT), followed by R1, then a filler of 10Ns,
#34 and then R2 ending with the mspI restriction site (CCG). The start and end of the sequence are buffered with a single N
#35
#35 We can use simple shell commands to query this data.
#35 Find out how many lines in the file (this is double the number of sequences)
#c#35	$wc -l reference.fasta
#36 Find out how many sequences there are directly by counting lines that only start with the header character ">"
#c#36	$mawk '/>/' reference.fasta | wc -l 
#37 We can test that all sequences follow the expected format.
#c#37	$mawk '/^NAATT.*N*.*CCGN$/' reference.fasta | wc -l
#c#37	$grep '^NAATT.*N*.*CCGN$' reference.fasta | wc -l
#37 No surprises here from our simulated data, but I highly recommend familiarizing yourself with grep, awk, and regular expressions to help evaluate de novo references.
#38
#38 Bonus Section
#38 
#38 Here, I am going to let you in on an experimental script I have been using to help optimize reference assemblies.
#38
#c#38	$curl -L -O https://raw.githubusercontent.com/jpuritz/WinterSchool.2016/master/RefMapOpt.sh
#38
#38 This script assembles references across cutoff values and then maps 20 random samples and evaluates mappings to the reference, along with number of contigs and coverage.  
#39 It takes a long time to run, but here's a sample command and output
#39
#c#39	$#RefMapOpt.sh 4 8 4 8 0.9 64 PE
#39
#39 This would loop across cutoffs of 4-8 using a similarity of 90% for clustering, parellized across 64 processors, using PE assembly technique.

#40 The output is stored in a file called `mapping.results`
#c#40	$cat mapping.results

#40 Cov		Non0Cov	Contigs	MeanContigsMapped	K1	K2	SUM Mapped	SUM Properly	Mean Mapped	Mean Properly	MisMatched
#40 37.3272	39.6102	1000	943.35				4	4	747290		747065			37364.5		37353.2			0
#40 37.3819	39.6683	1000	943.35				4	5	748386		748110			37419.3		37405.5			0
#40 37.4439	39.7298	1000	943.45				4	6	749626		749445			37481.3		37472.2			0
#40 37.4826	39.7709	1000	943.45				4	7	750401		750222			37520.1		37511.1			0
#40 37.4648	39.7522	1000	943.45				4	8	750045		749760			37502.2		37488			0
#40 37.3382	39.6198	1000	943.4				5	4	747510		747285			37375.5		37364.2			0
#40 37.3954	39.6805	1000	943.4				5	5	748655		748379			37432.8		37418.9			0
#40 37.448	39.7343	1000	943.45				5	6	749710		749529			37485.5		37476.4			0
#40 37.4689	39.7565	1000	943.45				5	7	750127		749860			37506.3		37493			0
#40 37.4436	39.728	999		942.55				5	8	748872		748587			37443.6		37429.3			0
#40 37.3447	39.6268	1000	943.4				6	4	747640		747433			37382		37371.7			0
#40 37.4288	39.7117	1000	943.5				6	5	749324		749220			37466.2		37461			0
#40 37.4724	39.7582	1000	943.5				6	6	750198		749944			37509.9		37497.2			0
#40 37.4537	39.7405	1000	943.45				6	7	749824		749557			37491.2		37477.8			0
#40 37.4381	39.72	999		942.6				6	8	748761		748478			37438.1		37423.9			0
#40 37.3876	39.6723	1000	943.4				7	4	748499		748407			37424.9		37420.3			0
#40 37.4374	39.7253	1000	943.4				7	5	749497		749388			37474.8		37469.4			0
#40 37.4482	39.7346	1000	943.45				7	6	749714		749475			37485.7		37473.8			0
#40 37.4117	39.6938	1000	943.5				7	7	748982		748732			37449.1		37436.6			0
#40 37.4108	39.6893	998		941.7				7	8	747468		747207			37373.4		37360.3			0
#40 37.4254	39.7041	1000	943.6				8	4	749256		749164			37462.8		37458.2			0
#40 37.4307	39.7202	1000	943.35				8	5	749363		749185			37468.2		37459.2			0
#40 37.4209	39.7042	998		941.6				8	6	747669		747400			37383.4		37370			0
#40 37.4094	39.6924	997		940.65				8	7	746692		746416			37334.6		37320.8			0
#40 37.4915	39.7712	989		933.3				8	8	742332		742048			37116.6		37102.4			0
#40
#40 I have added extra tabs for readability.  The output contains the average coverage per contig, 
#40 the average coverage per contig not counting zero coverage contigs, the number of contigs, 
#40 the mean number of contigs mapped, the two cutoff values used, the sum of all mapped reads, 
#40 the sum of all properly mapped reads, the mean number of mapped reads, the mean number of properly mapped reads, 
#40 and the number of reads that are mapped to mismatching contigs.
#40 Here, we are looking to values that maximize properly mapped reads, the mean number of contigs mapped, and the coverage.  
#40 In this example, it's easy.  Values 4,7 produce the highes number of properly mapped reads, coverage, and contigs. 
#41 Real data will involve a judgement call.  Again, I haven't finished vetting this script, so use at your own risk.
###########################################

#!/bin/bash
if which RefTut &>/dev/null; then
    LOC=$(which RefTut)
else
	LOC="./RefTut"
fi
if [[ -z "$1" ]]; then
head -16 $LOC | grep "##" 
else
PATTERN=#$1[[:blank:]]
PATTERN2=#c#$1
PATTERN4=^$1
grep $PATTERN $LOC | sed 's/'$PATTERN2'\t/ /g' | sed 's/#'$1'\s/ /g'
fi